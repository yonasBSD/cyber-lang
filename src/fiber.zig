// Copyright (c) 2023 Cyber (See LICENSE)

/// Fibers.

const std = @import("std");
const builtin = @import("builtin");
const build_options = @import("build_options");
const stdx = @import("stdx");
const t = stdx.testing;
const cy = @import("cyber.zig");
const vmc = @import("vm_c.zig");
const bc = @import("bc_gen.zig");
const rt = cy.rt;
const log = cy.log.scoped(.fiber);
const Value = cy.Value;
const bt = cy.types.BuiltinTypes;

pub const PanicPayload = u64;

pub const PanicType = enum(u8) {
    uncaughtError = vmc.PANIC_UNCAUGHT_ERROR,
    staticMsg = vmc.PANIC_STATIC_MSG,
    msg = vmc.PANIC_MSG,
    nativeThrow = vmc.PANIC_NATIVE_THROW,
    inflightOom = vmc.PANIC_INFLIGHT_OOM,
    none = vmc.PANIC_NONE,
};

test "fiber internals." {
    if (cy.is32Bit) {
        try t.eq(@sizeOf(vmc.Fiber), 64);
    } else {
        try t.eq(@sizeOf(vmc.Fiber), 72);
    }
}

pub fn allocFiber(vm: *cy.VM, pc: usize, args: []const cy.Value, argDst: u8, initialStackSize: u32) !cy.Value {
    // Args are copied over to the new stack.
    var stack = try vm.alloc.alloc(Value, initialStackSize);
    // Assumes initial stack size generated by compiler is enough to hold captured args.
    @memcpy(stack[argDst..argDst+args.len], args);

    const obj: *vmc.Fiber = @ptrCast(try cy.heap.allocExternalObject(vm, @sizeOf(vmc.Fiber), true));
    const parentDstLocal = cy.NullU8;
    obj.* = .{
        .typeId = bt.Fiber | vmc.CYC_TYPE_MASK,
        .rc = 1,
        .stackPtr = @ptrCast(stack.ptr),
        .stackLen = @intCast(stack.len),
        .pcOffset = @intCast(pc),
        .argStart = argDst,
        .numArgs = @intCast(args.len),
        .stackOffset = 0,
        .parentDstLocal = parentDstLocal,
        .throwTracePtr = undefined,
        .throwTraceCap = 0,
        .throwTraceLen = 0,
        .initialPcOffset = @intCast(pc),
        .panicPayload = undefined,
        .panicType = vmc.PANIC_NONE,
        .prevFiber = undefined,
        .stack_size = @intCast(initialStackSize),
    };

    return Value.initCycPtr(obj);
}

pub fn saveCurFiber(vm: *cy.VM) void {
    vm.c.curFiber.stackPtr = @ptrCast(vm.c.stack);
    vm.c.curFiber.stackLen = @intCast(vm.c.stack_len);

    const pc_off = cy.fiber.getInstOffset(vm.c.ops, vm.c.pc);
    vm.c.curFiber.pcOffset = @intCast(pc_off);

    const fp_off = cy.fiber.getStackOffset(vm.c.stack, vm.c.framePtr);
    vm.c.curFiber.stackOffset = @intCast(fp_off);
}

/// Since this is called from a coresume expression, the fiber should already be retained.
pub fn pushFiber(vm: *cy.VM, curFiberEndPc: usize, curFramePtr: [*]Value, fiber: *cy.Fiber, parentDstLocal: u8) PcFp {
    // Save current fiber.
    vm.c.curFiber.stackPtr = @ptrCast(vm.c.stack);
    vm.c.curFiber.stackLen = @intCast(vm.c.stack_len);
    vm.c.curFiber.pcOffset = @intCast(curFiberEndPc);
    vm.c.curFiber.stackOffset = @intCast(getStackOffset(vm.c.stack, curFramePtr));

    // Push new fiber.
    fiber.prevFiber = vm.c.curFiber;
    fiber.parentDstLocal = parentDstLocal;
    vm.c.curFiber = fiber;
    vm.c.stack = @ptrCast(fiber.stackPtr);
    vm.c.stack_len = fiber.stackLen;
    vm.c.stackEndPtr = vm.c.stack + fiber.stackLen;
    // Check if fiber was previously yielded.
    if (vm.c.ops[fiber.pcOffset].opcode() == .coyield) {
        log.tracev("fiber set to {} {*}", .{fiber.pcOffset + 3, vm.c.framePtr});
        return .{
            .pc = toVmPc(vm, fiber.pcOffset + 3),
            .fp = @ptrCast(fiber.stackPtr + fiber.stackOffset),
        };
    } else {
        log.tracev("fiber set to {} {*}", .{fiber.pcOffset, vm.c.framePtr});
        return .{
            .pc = toVmPc(vm, fiber.pcOffset),
            .fp = @ptrCast(fiber.stackPtr + fiber.stackOffset),
        };
    }
}

pub fn popFiber(vm: *cy.VM, cur: PcFpOff, retValue: Value) PcFpOff {
    vm.c.curFiber.stackPtr = @ptrCast(vm.c.stack);
    vm.c.curFiber.stackLen = @intCast(vm.c.stack_len);
    vm.c.curFiber.pcOffset = cur.pc;
    vm.c.curFiber.stackOffset = cur.fp;
    const dstLocal = vm.c.curFiber.parentDstLocal;

    // Release current fiber.
    const nextFiber = vm.c.curFiber.prevFiber.?;
    cy.arc.releaseObject(vm, cy.ptrAlignCast(*cy.HeapObject, vm.c.curFiber));

    // Set to next fiber.
    vm.c.curFiber = nextFiber;

    // Copy return value to parent local.
    if (dstLocal != cy.NullU8) {
        vm.c.curFiber.stackPtr[vm.c.curFiber.stackOffset + dstLocal] = @bitCast(retValue);
    } else {
        cy.arc.release(vm, retValue);
    }

    vm.c.stack = @ptrCast(vm.c.curFiber.stackPtr);
    vm.c.stack_len = vm.c.curFiber.stackLen;
    vm.c.stackEndPtr = vm.c.stack + vm.c.curFiber.stackLen;
    log.tracev("fiber set to {} {*}", .{vm.c.curFiber.pcOffset, vm.c.framePtr});
    return PcFpOff{
        .pc = vm.c.curFiber.pcOffset,
        .fp = vm.c.curFiber.stackOffset,
    };
}

/// Unwinds the stack and releases the locals.
/// This also releases the initial captured vars since it's on the stack.
pub fn releaseFiberStack(vm: *cy.VM, fiber: *cy.Fiber) !void {
    log.tracev("release fiber stack, start", .{});
    defer log.tracev("release fiber stack, end", .{});
    var stack = @as([*]Value, @ptrCast(fiber.stackPtr))[0..fiber.stackLen];
    const framePtr = fiber.stackOffset;
    const pc = fiber.pcOffset;

    if (pc != cy.NullId) {
        // Cleanup on main fiber block.
        if (vm.c.ops[pc].opcode() != .coreturn) {
            const symIdx = try cy.debug.indexOfDebugSym(vm, pc);
            const key = cy.debug.getUnwindKey(vm, symIdx);
            if (!key.is_null) {
                cy.arc.runUnwindReleases(vm, stack.ptr + framePtr, key);
            }

            // No end locals for main fiber block yet.
            // const endLocalsPc = cy.debug.debugSymToEndLocalsPc(vm, sym);
            // if (endLocalsPc != cy.NullId) {
            //     cy.arc.runBlockEndReleaseOps(vm, stack, framePtr, endLocalsPc);
            // }
        }
    }

    // Release any binded args.
    if (fiber.numArgs > 0) {
        for (stack[fiber.argStart..fiber.argStart+fiber.numArgs]) |arg| {
            log.tracev("release fiber arg", .{});
            cy.arc.release(vm, arg);
        }
    }

    // Finally free stack.
    vm.alloc.free(stack);
}

// Determine whether it's a vm or host frame.
pub fn getFrameType(_: *cy.VM, stack: []const Value, fp_off: u32) FrameType {
    if (fp_off == 0) {
        return .main;
    }
    return stack[fp_off+1].call_info.type;
}

/// Unwind from `ctx` and release each frame.
/// TODO: See if releaseFiberStack can resuse the same code.
pub fn unwindStack(vm: *cy.VM, stack: []const Value, ctx: PcFpOff) !PcFpOff {
    log.tracev("panic unwind {*}", .{stack.ptr + ctx.fp});
    var pc = ctx.pc;
    var fp = ctx.fp;

    vm.compactTrace.clearRetainingCapacity();

    while (true) {
        const frame_t = getFrameType(vm, stack, fp);
        log.tracev("unwind on frame pc={}, fp={}, type={}", .{pc, fp, frame_t});
        if (fp == 0 or frame_t == .vm) {
            try vm.compactTrace.append(vm.alloc, .{
                .pcOffset = pc,
                .fpOffset = fp,
            });
            try releaseFrame(vm, fp, pc);
            if (fp == 0) {
                // Done, at main block.
                return PcFpOff{ .pc = pc, .fp = fp };
            } else {
                const prev = getPrevFrame(vm, stack, fp);
                pc = prev.pc;
                fp = prev.fp;
            }
        } else {
            // Record host frames.
            if (frame_t == .host) {
                try vm.compactTrace.append(vm.alloc, .{
                    .pcOffset = cy.NullId,
                    .fpOffset = fp,
                });
            }
            // Skip frame.
            const prev = getPrevFrame(vm, stack, fp);
            pc = prev.pc;
            fp = prev.fp;
        }
    }
}

/// Walks the stack and records each frame.
pub fn recordCurFrames(vm: *cy.VM) !void {
    @branchHint(.cold);
    log.tracev("recordCompactFrames", .{});

    var fp = cy.fiber.getStackOffset(vm.c.stack, vm.c.framePtr);
    var pc = cy.fiber.getInstOffset(vm.c.ops, vm.c.pc);
    while (true) {
        const frame_t = getFrameType(vm, vm.c.getStack(), fp);
        log.tracev("record on pc={}, fp={}, type={}", .{pc, fp, frame_t});

        if (fp == 0 or frame_t == .vm) {
            try vm.compactTrace.append(vm.alloc, .{
                .pcOffset = pc,
                .fpOffset = fp,
            });
            if (fp == 0) {
                // Main.
                break;
            } else {
                const prev = getPrevFrame(vm, vm.c.getStack(), fp);
                fp = prev.fp;
                pc = prev.pc;
            }
        } else {
            if (frame_t == .host) {
                try vm.compactTrace.append(vm.alloc, .{
                    .pcOffset = cy.NullId,
                    .fpOffset = fp,
                });
            }
            const prev = getPrevFrame(vm, vm.c.getStack(), fp);
            fp = prev.fp;
            pc = prev.pc;
        }
    }
}

fn releaseFrame(vm: *cy.VM, fp: u32, pc: u32) !void {
    const symIdx = try cy.debug.indexOfDebugSym(vm, pc);
    const key = cy.debug.getUnwindKey(vm, symIdx);
    log.tracev("release frame: pc={} {}, fp={} key: {},{}", .{pc, vm.c.ops[pc].opcode(), fp, !key.is_null, key.idx});
    if (!key.is_null) {
        cy.arc.runUnwindReleases(vm, vm.c.stack + fp, key);
    }
}

fn releaseFrameTemps(vm: *cy.VM, fp: u32, pc: u32) !void {
    const symIdx = try cy.debug.indexOfDebugSym(vm, pc);
    const tempIdx = cy.debug.getDebugTempIndex(vm, symIdx);
    log.tracev("release frame temps: {} {}, tempIdx: {}", .{pc, vm.c.ops[pc].opcode(), tempIdx});
    defer log.tracev("release frame temps: end", .{});

    // Release temps.
    cy.arc.runTempReleaseOps(vm, vm.c.stack + fp, tempIdx);
}

fn getPrevFrame(vm: *cy.VM, stack: []const Value, fp: u32) PcFpOff {
    const prev_pc = getInstOffset(vm.c.ops, stack[fp + 2].retPcPtr) - stack[fp + 1].call_info.call_inst_off;
    const prev_fp = getStackOffset(stack.ptr, stack[fp + 3].retFramePtr);
    return PcFpOff{ .pc = prev_pc, .fp = prev_fp };
}

// Returns a continuation if there is a parent fiber otherwise null.
pub fn fiberEnd(vm: *cy.VM, ctx: PcFpOff) ?PcFpOff {
    if (vm.c.curFiber != &vm.c.mainFiber) {
        if (cy.Trace) {
            // Print fiber panic.
            cy.debug.printLastUserPanicError(vm) catch cy.fatal();
        }
        return cy.fiber.popFiber(vm, ctx, Value.initInt(0));
    } else return null;
}

pub const UnwindTry = packed struct {
    catch_pc: u32,
    prev: UnwindKey,
};

pub const UnwindKey = packed struct {
    idx: u30,
    is_try: bool,
    is_null: bool,

    pub fn initNull() UnwindKey {
        return .{ .idx = undefined, .is_try = undefined, .is_null = true };
    }

    pub fn fromCreatedEntry(entry: bc.UnwindEntry) UnwindKey {
        return .{
            .idx = entry.rt_idx,
            .is_try = entry.type == .try_e,
            .is_null = false,
        };
    }
};

/// Throws an error value by unwinding until the either the first matching catch block
/// is reached or `endFp` is reached.
/// If the main rootFp is reached without a catch block, the error is elevated to an uncaught panic error.
/// Records frames in `compactTrace`.
pub fn throw(vm: *cy.VM, endFp: u32, ctx: PcFpOff, err: Value) !PcFpOff {
    log.tracev("throw", .{});

    vm.compactTrace.clearRetainingCapacity();
    var fp = ctx.fp;
    var pc = ctx.pc;

    while (true) {
        try vm.compactTrace.append(vm.alloc, .{
            .pcOffset = pc,
            .fpOffset = fp,
        });

        const frame_t = getFrameType(vm, vm.c.getStack(), fp);
        log.tracev("throw on frame: fp={}, pc={}, type={}", .{fp, pc, frame_t});
        if (frame_t != .vm and frame_t != .main) {
            const prev = getPrevFrame(vm, vm.c.getStack(), fp);
            fp = prev.fp;
            pc = prev.pc;
            continue;
        }

        const debug_idx = try cy.debug.indexOfDebugSym(vm, pc);
        const key = cy.debug.getUnwindKey(vm, debug_idx);
        if (cy.arc.runUnwindReleasesUntilCatch(vm, vm.c.stack + fp, key)) |catch_pc| {
            const err_local = vm.c.ops[catch_pc+3].val;
            const release = vm.c.ops[catch_pc+4].val == 1;
            log.tracev("catch at {}, err={}, release={}, fp={}", .{catch_pc, err_local, release, fp});
            // Copy error to catch dst.
            if (err_local != cy.NullU8) {
                if (release) {
                    cy.arc.release(vm, vm.c.stack[fp + err_local]);
                }
                vm.c.stack[fp + err_local] = @bitCast(err);
            }
            // Goto catch block in the current frame.
            return PcFpOff{
                .pc = catch_pc + 5,
                .fp = fp,
            };
        }
        if (fp == endFp) {
            if (fp == 0) {
                // Main root. Convert to uncaught panic.
                vm.c.curFiber.panicType = vmc.PANIC_UNCAUGHT_ERROR;

                // Build stack trace.
                const frames = try cy.debug.allocStackTrace(vm, vm.c.getStack(), vm.compactTrace.items());
                vm.stackTrace.deinit(vm.alloc);
                vm.stackTrace.frames = frames;

                return fiberEnd(vm, .{ .pc = pc, .fp = fp }) orelse {
                    return error.Panic;
                };
            } else {
                // Gives host dispatch a chance to catch the error.
                vm.c.curFiber.panicType = vmc.PANIC_UNCAUGHT_ERROR;
                return error.Panic;
            }
        }

        const prev = getPrevFrame(vm, vm.c.getStack(), fp);
        fp = prev.fp;
        pc = prev.pc;
    }
}

pub fn freeFiberPanic(vm: *const cy.VM, fiber: *vmc.Fiber) void {
    const panicT: PanicType = @enumFromInt(fiber.panicType);
    if (panicT == .msg) {
        const ptr: usize = @intCast(fiber.panicPayload & ((1 << 48) - 1));
        const len: usize = @intCast(fiber.panicPayload >> 48);
        vm.alloc.free(@as([*]const u8, @ptrFromInt(ptr))[0..len]);
    }
}

pub inline fn getInstOffset(from: [*]const cy.Inst, to: [*]const cy.Inst) u32 {
    return @intCast(@intFromPtr(to) - @intFromPtr(from));
}

pub inline fn getStackOffset(from: [*]const Value, to: [*]const Value) u32 {
    // Divide by eight.
    return @intCast((@intFromPtr(to) - @intFromPtr(from)) >> 3);
}

pub inline fn stackEnsureUnusedCapacity(self: *cy.VM, unused: u32) !void {
    if (@intFromPtr(self.framePtr) + 8 * unused >= @intFromPtr(self.stack.ptr + self.stack.len)) {
        try self.stackGrowTotalCapacity((@intFromPtr(self.framePtr) + 8 * unused) / 8);
    }
}

pub inline fn stackEnsureTotalCapacity(self: *cy.VM, newCap: usize) !void {
    if (newCap > self.c.stack_len) {
        try stackGrowTotalCapacity(self, newCap);
    }
}

pub fn stackEnsureTotalCapacityPrecise(self: *cy.VM, newCap: usize) !void {
    if (newCap > self.c.stack_len) {
        try stackGrowTotalCapacityPrecise(self, newCap);
    }
}

pub fn stackGrowTotalCapacity(self: *cy.VM, newCap: usize) !void {
    var betterCap = self.c.stack_len;
    while (true) {
        betterCap +|= betterCap / 2 + 8;
        if (betterCap >= newCap) {
            break;
        }
    }
    try stackGrowTotalCapacityPrecise(self, betterCap);
}

pub fn stackGrowTotalCapacityPrecise(self: *cy.VM, newCap: usize) !void {
    if (self.alloc.resize(self.c.getStack(), newCap)) {
        self.c.stack_len = newCap;
        self.c.stackEndPtr = self.c.stack + newCap;
    } else {
        const new_stack = try self.alloc.realloc(self.c.getStack(), newCap);
        self.c.stack = new_stack.ptr;
        self.c.stack_len = new_stack.len;
        self.c.stackEndPtr = self.c.stack + newCap;

        if (builtin.is_test or cy.Trace) {
            // Fill the stack with null heap objects to surface undefined access better.
            @memset(self.c.getStack(), Value.initCycPtr(&DummyHeapObject));
        }
    }
}

var DummyHeapObject = cy.HeapObject{
    .head = .{
        .typeId = cy.NullId,
        .rc = 0,
    },
};

pub inline fn toVmPc(self: *const cy.VM, offset: usize) [*]cy.Inst {
    return self.c.ops + offset;
}

// Performs stackGrowTotalCapacityPrecise in addition to patching the frame pointers.
pub fn growStackAuto(vm: *cy.VM) !void {
    @branchHint(.cold);
    log.tracev("grow stack", .{});
    // Grow by 50% with minimum of 16.
    var growSize = vm.c.stack_len / 2;
    if (growSize < 16) {
        growSize = 16;
    }
    try growStackPrecise(vm, vm.c.stack_len + growSize);
}

pub fn ensureTotalStackCapacity(vm: *cy.VM, newCap: usize) !void {
    if (newCap > vm.c.stack_len) {
        var betterCap = vm.c.stack_len;
        while (true) {
            betterCap +|= betterCap / 2 + 8;
            if (betterCap >= newCap) {
                break;
            }
        }
        try growStackPrecise(vm, betterCap);
    }
}

fn growStackPrecise(vm: *cy.VM, newCap: usize) !void {
    if (vm.alloc.resize(vm.c.getStack(), newCap)) {
        vm.c.stack_len = newCap;
        vm.c.stackEndPtr = vm.c.stack + newCap;
    } else {
        const newStack = try vm.alloc.alloc(Value, newCap);

        // Copy to new stack.
        @memcpy(newStack[0..vm.c.stack_len], vm.c.stack);

        // Patch frame ptrs. 
        var curFpOffset = getStackOffset(vm.c.stack, vm.c.framePtr);
        while (curFpOffset != 0) {
            const prevFpOffset = getStackOffset(vm.c.stack, newStack[curFpOffset + 3].retFramePtr);
            newStack[curFpOffset + 3].retFramePtr = newStack.ptr + prevFpOffset;
            curFpOffset = prevFpOffset;
        }

        // Free old stack.
        vm.alloc.free(vm.c.getStack());

        // Update to new frame ptr.
        vm.c.framePtr = newStack.ptr + getStackOffset(vm.c.stack, vm.c.framePtr);
        vm.c.stack = newStack.ptr;
        vm.c.stack_len = newStack.len;
        vm.c.stackEndPtr = vm.c.stack + newCap;
    }
}

pub const PcFpOff = struct {
    pc: u32,
    fp: u32,
};

pub const PcFp = struct {
    pc: [*]cy.Inst,
    fp: [*]Value,
};

pub const FrameType = enum(u2) {
    vm,
    host,
    dyn,
    main,
};

pub const CallInfo = packed struct {
    /// Whether a ret op should return from the VM loop.
    ret_flag: bool,

    /// Since there are different call insts with varying lengths,
    /// the call convention prefers to advance the pc before saving it so
    /// stepping over the call will already have the correct pc.
    /// An offset is stored to the original call inst for stack unwinding.
    call_inst_off: u7,

    /// Stack size of the function.
    /// Used to dynamically push stack frames when calling into the VM.
    stack_size: u8,

    type: FrameType,
    padding: u14 = undefined,
    payload: u32 = undefined,
};
